{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:42:25.527651Z",
     "start_time": "2020-02-27T17:42:25.523125Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import operator\n",
    "import copy, random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from itertools import cycle\n",
    "from math import pi\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 0: Provide the path to the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:42:30.507542Z",
     "start_time": "2020-02-27T17:42:30.505376Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "your_path = '../../data'\n",
    "path = \"{}/valentine-paper-results/output/\".format(your_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 1: Rename files \n",
    "> **Note**: Only run this if you have new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T08:22:31.734504Z",
     "start_time": "2020-02-27T08:22:30.133325Z"
    },
    "hidden": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "for folder in os.listdir(path):\n",
    "    if folder.startswith('.'):\n",
    "        continue\n",
    "    for file in os.listdir(path+folder):\n",
    "        filename = folder+'/'+file\n",
    "        if filename.endswith(\".json\"):\n",
    "            files.append('.'.join(filename.split('.')[:-1]))\n",
    "\n",
    "for f in files:\n",
    "    g = f.replace('EmbDI_with_cid_all_with_rid_first_flatten_all_', \"EmbDI{'with_cid': 'all', 'with_rid': 'first', 'flatten': 'all'}\")\n",
    "    g = g.replace('evCupid','ev__Cupid')\n",
    "    g = g.replace('__CorrelationClustering','__DistributionBased')\n",
    "    g = g.replace('evSimilarityFlooding','ev__SimilarityFlooding')\n",
    "    g = g.replace('evJaccardLevenMatcher','ev__JaccardLevenMatcher')\n",
    "    if 'COMA_OPT_INST' in g:\n",
    "        g = g.replace('__Coma','__COMA-SI')\n",
    "    else:\n",
    "        g = g.replace('__Coma','__COMA-S')\n",
    "\n",
    "\n",
    "#     print(g)\n",
    "    os.rename(path+f+'.json',path+g+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 2: Read files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Parse the result files and present a table with the precision and recall metrics for all the experiments\n",
    "\n",
    "for every algorithm create a plot for each metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Code below read the output of the framework and creates a dict of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:42:35.622674Z",
     "start_time": "2020-02-27T17:42:35.583040Z"
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_metrics = {}\n",
    "run_times = {}\n",
    "\n",
    "def show_the_output(path: str):\n",
    "    \n",
    "    with open('%s'%(path),'r') as input:\n",
    "        split_path = path.split('/')\n",
    "        filename = '.'.join(split_path[len(split_path)-1].split('.')[:-1])\n",
    "        lines = json.load(input)\n",
    "        total_metrics[filename] = lines['metrics']\n",
    "        run_times[filename] = lines['run_times']['total_time']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:43:10.242736Z",
     "start_time": "2020-02-27T17:42:39.175212Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68023/68023 [00:12<00:00, 5551.65it/s]\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for folder in os.listdir(path):\n",
    "    if folder.startswith('.'):\n",
    "        continue\n",
    "    for file in os.listdir(\"{}{}\".format(path, folder)):\n",
    "        filename = path+folder+'/'+file\n",
    "        if filename.endswith(\".json\"):\n",
    "            files.append(filename)\n",
    "\n",
    "files.sort()\n",
    "for file in tqdm(files):\n",
    "    try:\n",
    "        show_the_output(file)\n",
    "    except:\n",
    "        raise Exception(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 3: Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:43:15.079378Z",
     "start_time": "2020-02-27T17:43:15.073330Z"
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_best_config_dict(source_dict: dict, target_dict: dict, to_table: dict):\n",
    "    for key in source_dict.keys():\n",
    "        for algo in source_dict[key].keys():\n",
    "            max_item = max(source_dict[key][algo].items(), key=operator.itemgetter(1))\n",
    "            target_dict[key][algo] = max_item[0]\n",
    "            to_table[key][algo] = max_item[1]\n",
    "\n",
    "def get_best_metric(source_dict: dict, target_dict: dict, index):\n",
    "    for key in source_dict.keys():\n",
    "        for algo in source_dict[key].keys():\n",
    "            target_dict[key][algo] = source_dict[key][algo][index[key][algo]]\n",
    "            \n",
    "def add_variable_columns(frame: pd.DataFrame, category: list, mother_table: list, way: list, \n",
    "                         horizontal_overlap: list, vertical_overlap: list, column_names: list, typeOfValues: list):\n",
    "    frame['Category'] = category\n",
    "    frame['MotherTable'] = mother_table\n",
    "    frame['SplitMethod'] = way\n",
    "    frame['HorizontalOverlap'] = horizontal_overlap\n",
    "    frame['VerticalOverlap'] = vertical_overlap\n",
    "    frame['ColumnNames'] = column_names\n",
    "    frame['TypeOfValues'] = typeOfValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:43:23.644801Z",
     "start_time": "2020-02-27T17:43:20.055281Z"
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1_score = {}\n",
    "precision_at_10_percent = {}\n",
    "precision_at_20_percent = {}\n",
    "precision_at_30_percent = {}\n",
    "precision_at_40_percent = {}\n",
    "precision_at_50_percent = {}\n",
    "precision_at_60_percent = {}\n",
    "precision_at_70_percent = {}\n",
    "precision_at_80_percent = {}\n",
    "precision_at_90_percent = {}\n",
    "recall_at_sizeof_ground_truth = {}\n",
    "run_time = {}\n",
    "\n",
    " \n",
    "algorithms = {\n",
    "    'Cupid': None,\n",
    "    'DistributionBased': None,\n",
    "    'SimilarityFlooding': None,\n",
    "    'SemProp': None,\n",
    "    'JaccardLevenMatcher':None,\n",
    "    'COMA-S': None,\n",
    "    'COMA-SI': None,\n",
    "    'EmbDI': None\n",
    "}\n",
    "\n",
    "problem_dictionary = {\n",
    "    'Unionable': ['horizontal','unionable'],\n",
    "    'View-Unionable': ['both_0_', 'viewunion'],\n",
    "    'Joinable': ['both_50_', 'vertical','_joinable'],\n",
    "    'Semantically-Joinable': ['both_50_','vertical','_semjoinable'] # TODO: change with the correct file convention\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for key in total_metrics.keys():\n",
    "    precision[key.split('__')[0]] = {}\n",
    "    recall[key.split('__')[0]] = {}\n",
    "    f1_score[key.split('__')[0]] = {}\n",
    "    precision_at_10_percent[key.split('__')[0]] = {}\n",
    "    precision_at_20_percent[key.split('__')[0]] = {}\n",
    "    precision_at_30_percent[key.split('__')[0]] = {}\n",
    "    precision_at_40_percent[key.split('__')[0]] = {}\n",
    "    precision_at_50_percent[key.split('__')[0]] = {}\n",
    "    precision_at_60_percent[key.split('__')[0]] = {}\n",
    "    precision_at_70_percent[key.split('__')[0]] = {}\n",
    "    precision_at_80_percent[key.split('__')[0]] = {}\n",
    "    precision_at_90_percent[key.split('__')[0]] = {}\n",
    "    recall_at_sizeof_ground_truth[key.split('__')[0]] = {}\n",
    "    run_time[key.split('__')[0]] = {}\n",
    "\n",
    "        \n",
    "for key in total_metrics.keys():\n",
    "    if not \"precision_at_n_percent\" in total_metrics[key].keys():\n",
    "#         print(key)\n",
    "#         print(key.split('__')[1].split('{')[0])\n",
    "        precision[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        f1_score[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        recall[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_10_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_20_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_30_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_40_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_50_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_60_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_70_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_80_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        precision_at_90_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        recall_at_sizeof_ground_truth[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "        run_time[key.split('__')[0]][key.split('__')[1].split('{')[0]] = {}\n",
    "\n",
    "\n",
    "for key in total_metrics.keys():\n",
    "    if not \"precision_at_n_percent\" in total_metrics[key].keys():\n",
    "        if len(key.split('__')[1].split('{')) > 1:\n",
    "            key_params = '{'+key.split('__')[1].split('{')[1]\n",
    "        else:\n",
    "            key_params = \"generic\"\n",
    "        precision[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision']\n",
    "        recall[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['recall']\n",
    "        f1_score[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['f1_score']\n",
    "        precision_at_10_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_10_percent']\n",
    "        precision_at_20_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_20_percent']\n",
    "        precision_at_30_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_30_percent']\n",
    "        precision_at_40_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_40_percent']\n",
    "        precision_at_50_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_50_percent']\n",
    "        precision_at_60_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_60_percent']\n",
    "        precision_at_70_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_70_percent']\n",
    "        precision_at_80_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_80_percent']\n",
    "        precision_at_90_percent[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['precision_at_90_percent']\n",
    "        recall_at_sizeof_ground_truth[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = total_metrics[key]['recall_at_sizeof_ground_truth']\n",
    "        run_time[key.split('__')[0]][key.split('__')[1].split('{')[0]][key_params] = run_times[key]\n",
    "\n",
    "\n",
    "best_dict = {}\n",
    "for dataset in precision.keys():\n",
    "    best_dict[dataset] = copy.deepcopy(algorithms)\n",
    "       \n",
    "best_table = copy.deepcopy(best_dict)\n",
    "find_best_config_dict(f1_score,best_dict,best_table)\n",
    "\n",
    "# print('\\n\\nBest Configuration 1-1\\n')\n",
    "best_configuration_121 = pd.DataFrame.from_dict(best_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "# display(best_configuration_121)\n",
    "\n",
    "# print('\\n\\nPrecision\\n')\n",
    "best_prec_dict = copy.deepcopy(best_dict)\n",
    "get_best_metric(precision,best_prec_dict,best_dict)\n",
    "# display(best_prec_dict)\n",
    "best_prec_pd = pd.DataFrame.from_dict(best_prec_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "best_prec_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "shape = best_prec_pd.shape\n",
    "random_list = np.random.uniform(low=0, high=1, size=shape[0])\n",
    "for i in best_prec_pd.columns:\n",
    "    m = best_prec_pd[i].isnull()\n",
    "    #count rows with NaNs\n",
    "    l = m.sum()\n",
    "    #create array with size l\n",
    "    s = np.random.choice(random_list, size=l)\n",
    "    #set NaNs values\n",
    "    best_prec_pd.loc[m, i] = s\n",
    "    \n",
    "\n",
    "category = []\n",
    "mother_table = []\n",
    "way = []\n",
    "column_names = []\n",
    "typeOfValues = []\n",
    "horizontal_overlap = []\n",
    "vertical_overlap = []\n",
    "\n",
    "for index, dataset in best_prec_pd.loc[:,['Dataset']].iterrows():\n",
    "    category_found = False\n",
    "    for problem in problem_dictionary.keys():\n",
    "        for ss in problem_dictionary[problem]:\n",
    "            if ss in dataset['Dataset']:\n",
    "                if (problem == 'Joinable' and '_ev' in dataset['Dataset']) or 'Musicians' in dataset['Dataset']:\n",
    "                    category.append(problem)\n",
    "                    category_found = True\n",
    "                elif (problem == 'Semantically-Joinable' and '_av' in dataset['Dataset']) or 'Musicians' in dataset['Dataset']:\n",
    "                    category.append(problem)\n",
    "                    category_found = True\n",
    "                elif not (problem == 'Joinable' or problem == 'Semantically-Joinable'):\n",
    "                    category.append(problem)\n",
    "                    category_found = True\n",
    "\n",
    "    if not category_found:\n",
    "        category.append(\"generic\")\n",
    "            \n",
    "    variables = dataset['Dataset'].split('_')\n",
    "#     print(variables)\n",
    "#     print(variables[1] == 'both')\n",
    "    mother_table.append(variables[0])\n",
    "    if variables[1] == 'both':\n",
    "        way.append(variables[1])\n",
    "        horizontal_overlap.append(variables[2])\n",
    "        vertical_overlap.append(variables[3])\n",
    "        column_names.append(variables[4])\n",
    "        typeOfValues.append(variables[5])\n",
    "    elif variables[1] == 'horizontal':\n",
    "        way.append(variables[1])\n",
    "        horizontal_overlap.append(variables[2])\n",
    "        vertical_overlap.append(None)\n",
    "        column_names.append(variables[3])\n",
    "        typeOfValues.append(variables[4])\n",
    "    elif variables[1] == 'vertical':\n",
    "        way.append(variables[1])\n",
    "        horizontal_overlap.append(None)\n",
    "        vertical_overlap.append(variables[2])\n",
    "        column_names.append(variables[3])\n",
    "        typeOfValues.append(variables[4])\n",
    "    else:\n",
    "        way.append(None)\n",
    "        horizontal_overlap.append(None)\n",
    "        vertical_overlap.append(None)\n",
    "        column_names.append(None)\n",
    "        typeOfValues.append(None)\n",
    "\n",
    "add_variable_columns(best_prec_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec_pd)\n",
    "\n",
    "# print('\\n\\nRecall\\n')\n",
    "best_recall_dict = copy.deepcopy(best_dict)\n",
    "get_best_metric(recall,best_recall_dict,best_dict)\n",
    "best_recall_pd = pd.DataFrame.from_dict(best_recall_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_recall_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_recall_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_recall_pd)\n",
    "\n",
    "# print('\\n\\nF1_score\\n')\n",
    "best_f1_dict = copy.deepcopy(best_dict)\n",
    "get_best_metric(f1_score,best_f1_dict,best_dict)\n",
    "best_f1_pd = pd.DataFrame.from_dict(best_f1_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_f1_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_f1_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_f1_pd)\n",
    "\n",
    "nm_table = copy.deepcopy(best_dict)\n",
    "nm_dict = copy.deepcopy(best_dict)\n",
    "find_best_config_dict(recall_at_sizeof_ground_truth,nm_dict,nm_table)\n",
    "\n",
    "# print('\\n\\nBest Configuration n-m\\n')\n",
    "best_configuration_nm = pd.DataFrame.from_dict(nm_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "# display(best_configuration_nm)\n",
    "\n",
    "# print('\\n\\nPrecision at 10 percent\\n')\n",
    "best_prec10_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_10_percent,best_prec10_dict,nm_dict)\n",
    "best_prec10_pd = pd.DataFrame.from_dict(best_prec10_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec10_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec10_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec10_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 20 percent\\n')\n",
    "best_prec20_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_20_percent,best_prec20_dict,nm_dict)\n",
    "best_prec20_pd = pd.DataFrame.from_dict(best_prec20_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec20_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec20_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec20_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 30 percent\\n')\n",
    "best_prec30_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_30_percent,best_prec30_dict,nm_dict)\n",
    "best_prec30_pd = pd.DataFrame.from_dict(best_prec30_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec30_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec30_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec30_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 40 percent\\n')\n",
    "best_prec40_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_40_percent,best_prec40_dict,nm_dict)\n",
    "best_prec40_pd = pd.DataFrame.from_dict(best_prec40_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec40_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec40_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec40_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 50 percent\\n')\n",
    "best_prec50_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_50_percent,best_prec50_dict,nm_dict)\n",
    "best_prec50_pd = pd.DataFrame.from_dict(best_prec50_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec50_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec50_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec50_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 60 percent\\n')\n",
    "best_prec60_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_60_percent,best_prec60_dict,nm_dict)\n",
    "best_prec60_pd = pd.DataFrame.from_dict(best_prec60_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec60_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec60_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec60_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 70 percent\\n')\n",
    "best_prec70_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_70_percent,best_prec70_dict,nm_dict)\n",
    "best_prec70_pd = pd.DataFrame.from_dict(best_prec70_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec70_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec70_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec70_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 80 percent\\n')\n",
    "best_prec80_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_80_percent,best_prec80_dict,nm_dict)\n",
    "best_prec80_pd = pd.DataFrame.from_dict(best_prec80_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec80_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec80_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec80_pd)\n",
    "\n",
    "# print('\\n\\nPrecision at 90 percent\\n')\n",
    "best_prec90_dict = copy.deepcopy(nm_dict)\n",
    "get_best_metric(precision_at_90_percent,best_prec90_dict,nm_dict)\n",
    "best_prec90_pd = pd.DataFrame.from_dict(best_prec90_dict, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_prec90_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_prec90_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_prec90_pd)\n",
    "\n",
    "# print('\\n\\nRecall at sizeof groundtruth\\n')\n",
    "best_rec_gnd_pd = pd.DataFrame.from_dict(nm_table, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_rec_gnd_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n",
    "best_rec_gnd_pd.fillna(value=pd.np.nan, inplace=True)\n",
    "# display(best_rec_gnd_pd)\n",
    "\n",
    "best_run_time = copy.deepcopy(nm_dict)\n",
    "get_best_metric(run_time, best_run_time, nm_dict)\n",
    "best_run_time_pd = pd.DataFrame.from_dict(best_run_time, orient='index').reset_index().rename(columns={\"index\": \"Dataset\"})\n",
    "add_variable_columns(best_run_time_pd,category,mother_table,way,horizontal_overlap,vertical_overlap,column_names, typeOfValues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# skip table pairs that were execluded because they had more than exactly one fk-relation\n",
    "skip = list(best_rec_gnd_pd[best_rec_gnd_pd['JaccardLevenMatcher'].isnull()][\"Dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# schemapile eval\n",
    "best_rec_gnd_pd[(best_rec_gnd_pd['Category'].isin([\"Joinable\",\"Semantically-Joinable\"])) &  (~best_rec_gnd_pd['Dataset'].isin(skip))].describe()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}